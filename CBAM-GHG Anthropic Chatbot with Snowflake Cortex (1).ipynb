{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "pvw5zaulfvnudyugal3p",
   "authorId": "3248219620606",
   "authorName": "MSMAISA",
   "authorEmail": "maria.zvezdkina@gmail.com",
   "sessionId": "2203655e-61c0-40f2-a88a-cb856c476801",
   "lastEditTime": 1762191167450
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2d13f5-e982-4f75-b5a0-41a7f0befc50",
   "metadata": {
    "collapsed": false,
    "name": "INTRO_MD",
    "resultHeight": 298
   },
   "source": [
    "# Getting Started with ‚ùÑÔ∏è Anthropic on Snowflake Cortex\n",
    "\n",
    "Build an intelligent question-answering system using Anthropic's Claude and Snowflake's AI capabilities.‚ö°Ô∏è\n",
    "\n",
    "This notebook demonstrates how to build an end-to-end application that:\n",
    "1. Processes PDF documents using Cortex Process Docouments\n",
    "2. Creates Cortex Search Service to do keyword and vector searches\n",
    "3. Implements a chat interface using Snowflake's Cortex and Anthropic's Claude in Streamlit\n",
    "\n",
    "Check out the [Quickstart](https://quickstarts.snowflake.com/guide/getting_started_on_anthropic_with_snowflake_cortex) for instructions on getting setup for this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292396c3-271e-4760-949d-a018ae2ecaae",
   "metadata": {
    "collapsed": false,
    "name": "SETUP_ENVIRONMENT_MD",
    "resultHeight": 172
   },
   "source": [
    "## Setting Up Your Environment üéí\n",
    "\n",
    "First, we'll import the required packages and set up our Snowflake session. The notebook uses several key packages:\n",
    "- `streamlit`: For creating the interactive chat interface\n",
    "- `snowflake-ml-python`: For Snowflake Cortex for embeddings and LLM capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "IMPORT_PACKAGES",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.cortex import complete, EmbedText768\n",
    "from snowflake.snowpark.types import VectorType, FloatType\n",
    "from snowflake.core.table import Table, TableColumn\n",
    "from snowflake.core import CreateMode, Root\n",
    "from snowflake.snowpark.functions import cast, col\n",
    "\n",
    "\n",
    "session = get_active_session()\n",
    "current_warehouse = session.get_current_warehouse()\n",
    "database_name = session.get_current_database()\n",
    "schema_name = session.get_current_schema()\n",
    "role_name = session.get_current_role()\n",
    "service_name = 'document_search_service'\n",
    "root = Root(session)\n",
    "database = root.databases[database_name]\n",
    "schema = database.schemas[schema_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf55b1e-27f2-4c48-b2f9-2c28a5fd6f26",
   "metadata": {
    "collapsed": false,
    "name": "SETUP_STAGE_VARIABLES_MD",
    "resultHeight": 102
   },
   "source": [
    "## Setting Up Stage Variables üìÅ\n",
    "\n",
    "We'll define our stage name and retrieve the list of files to process. This stage should contain the PDF documents we want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba83db-cebb-4135-b42e-cd914d04979c",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "VARIABLES",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "stage_name=\"@Documents\"\n",
    "files = session.sql(f\"LIST{stage_name}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1c904-ac79-4687-abbc-ff58476cbfdb",
   "metadata": {
    "collapsed": false,
    "name": "DOCUMENT_PROCESSING_MD",
    "resultHeight": 102
   },
   "source": [
    "## Document Processing Functions üìÑ\n",
    "\n",
    "We'll create functions to extract text from PDF files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb862d46-00a2-4c0f-b2f2-f2aa9fd4b932",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "DOCUMENT_PROCESSING",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "def process(file_name: str):\n",
    "    query = \"\"\"\n",
    "        SELECT TO_VARCHAR(\n",
    "            SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
    "                ?,\n",
    "                ?,\n",
    "                {'mode': 'OCR'}):content\n",
    "        ) AS OCR;\n",
    "    \"\"\"\n",
    "\n",
    "    resp = session.sql(query, params=[stage_name, file_name]).collect()\n",
    "    text = resp[0]['OCR']\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'TEXT' : [text],\n",
    "        'FILE_NAME': file_name\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf82656-1bcb-45ea-9922-431013d3b5eb",
   "metadata": {
    "collapsed": false,
    "name": "CREATE_EMBEDDINGS_MD",
    "resultHeight": 172
   },
   "source": [
    "## Processing Documents\n",
    "\n",
    "Now we'll:\n",
    "1. Process all documents in our stage\n",
    "2. Store the results in our table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca313e-5d8a-4eac-9566-7ab1ca5cc381",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "PROCESS_FILES",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Extract file names and process files\n",
    "file_names = [file['name'].split('/')[1] for file in files]\n",
    "\n",
    "# Download and process files into a DataFrame\n",
    "final_dataframe = pd.concat([\n",
    "    process(file_name)\n",
    "    for file_name in file_names\n",
    "], ignore_index=True)\n",
    "\n",
    "snowpark_df = session.create_dataframe(final_dataframe).select(\n",
    "    col(\"file_name\"),\n",
    "    col(\"text\")\n",
    ")\n",
    "\n",
    "# Write the transformed data directly to the target table\n",
    "snowpark_df.write.mode(\"overwrite\").save_as_table(\"docs_text_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd481dc-b606-4fec-8753-7759aabed213",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "## Create Cortex Search Service \n",
    "\n",
    "### Key Components Explained üìö\n",
    "\n",
    "#### Required Parameters\n",
    "\n",
    "- `ON`: Specifies the column containing the text to be indexed  \n",
    "- `ATTRIBUTES`: Additional columns to include in search results (e.g., file\\_name)  \n",
    "- `WAREHOUSE`: Compute warehouse for processing the embeddings  \n",
    "- `TARGET_LAG`: Maximum allowed lag for index updates  \n",
    "- `EMBEDDING_MODEL`: Model used to generate text embeddings  \n",
    "- Source query: The SELECT statement defining the data to index\n",
    "\n",
    "#### Configuration Options üîß\n",
    "\n",
    "1. Target Lag Settings:  \n",
    "     \n",
    "   - Shorter lag times mean more frequent updates  \n",
    "   - Common values: '1 hour', '1 day', '1 week'  \n",
    "   - Balance freshness needs with compute costs\n",
    "\n",
    "   \n",
    "\n",
    "2. Embedding Model Options:  \n",
    "     \n",
    "   - 'snowflake-arctic-embed-l-v2.0': Latest Snowflake embedding model  \n",
    "   - Optimized for English language content  \n",
    "   - 384-dimensional embeddings\n",
    "\n",
    "   \n",
    "\n",
    "3. Warehouse Considerations:  \n",
    "     \n",
    "   - Choose size based on data volume  \n",
    "   - Consider compute costs vs update frequency  \n",
    "   - Monitor warehouse utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe7945-d6a7-4309-a331-a67b1123238d",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CREATE_CORTEX_SEARCH_SERVICE",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE {{service_name}}\n",
    "  ON text\n",
    "  ATTRIBUTES file_name\n",
    "  WAREHOUSE = {{current_warehouse}}\n",
    "  TARGET_LAG = '1 day'\n",
    "  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n",
    "  AS (\n",
    "    SELECT\n",
    "        text,\n",
    "        file_name\n",
    "    FROM docs_text_table\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca153ba-92e0-4cd2-8e0c-343d781aead3",
   "metadata": {
    "collapsed": false,
    "name": "BUILD_CHAT_INTERFACE_MD",
    "resultHeight": 371
   },
   "source": [
    "## Building the Chat Interface üí¨\n",
    "\n",
    "Finally, we'll create our chat interface that uses:\n",
    "- Utilizes the Cortex Search Service for finding relevant context\n",
    "- Chat history management for conversation continuity\n",
    "- Anthropic's Claude model for generating responses\n",
    "- Streamlit for the user interface\n",
    "\n",
    "Key parameters:\n",
    "- `num_results`: Number of context results provided (default: 3)\n",
    "- `model_name`: Language model used (default: \"claude-4-sonnet\")\n",
    "- `history_length`: Chat history length (default: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc57aa3-082f-4cc4-a7b0-e01c56ee6d90",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "BUILD_CHAT_INTERFACE",
    "resultHeight": 3622
   },
   "outputs": [],
   "source": "import streamlit as st\nimport json\nimport time\nimport re\nimport requests\nimport pandas as pd\nfrom snowflake.core import Root\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col\nfrom snowflake.cortex import complete\n\n# Snowflake session setup\nsession = get_active_session()\nroot = Root(session)\ndatabase_name = session.get_current_database()\nschema_name = session.get_current_schema()\nservice_name = 'document_search_service'\n\n# Configuration\nmodel_name = \"claude-haiku-4-5\"\nnum_results = 3\nhistory_length = 5\nDEFAULT_CARBON_PRICE = 80.0\nMIN_REQUEST_INTERVAL = 2.0\nRETRY_DELAY = 5\n\nDEFAULT_EMISSIONS = {\n    \"steel\": 2.3,\n    \"aluminum\": 8.6,\n    \"cement\": 0.9,\n    \"fertilizer\": 1.5,\n    \"electricity\": 0.4\n}\n\ndef fetch_live_carbon_price():\n    \"\"\"Fetch EU carbon price using regex from Trading Economics HTML.\"\"\"\n    try:\n        url = \"https://tradingeconomics.com/commodity/carbon\"\n        response = requests.get(url, timeout=5)\n        match = re.search(r'<td[^>]*>EU Carbon Permits<\\/td>\\s*<td[^>]*>(\\d+\\.\\d+)', response.text)\n        if match:\n            return float(match.group(1))\n    except Exception as e:\n        st.warning(f\"‚ö†Ô∏è Could not fetch live carbon price: {e}\")\n    return DEFAULT_CARBON_PRICE\n\ndef write_live_price_to_stage(carbon_price):\n    \"\"\"Write live price to a .txt file and upload to @Documents stage.\"\"\"\n    content = f\"\"\"Live EU ETS Carbon Price: ‚Ç¨{carbon_price}/tonne CO‚ÇÇ as of today.\nSource: Trading Economics (https://tradingeconomics.com/commodity/carbon)\nThis price should be used for CBAM cost calculations unless overridden by official EU guidance.\"\"\"\n    with open(\"/tmp/live_price.txt\", \"w\") as f:\n        f.write(content)\n    session.file.put(\"/tmp/live_price.txt\", \"@Documents\", overwrite=True)\n\n\ndef process(file_name: str):\n    query = \"\"\"\n        SELECT TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(?, ?, {'mode': 'OCR'}):content) AS OCR;\n    \"\"\"\n    resp = session.sql(query, params=[\"@Documents\", file_name]).collect()\n    text = resp[0]['OCR']\n    return pd.DataFrame({'TEXT': [text], 'FILE_NAME': file_name})\n\ndef reindex_documents():\n    files = session.sql(\"LIST @Documents\").collect()\n    file_names = [file['name'].split('/')[-1] for file in files]\n    final_dataframe = pd.concat([process(name) for name in file_names], ignore_index=True)\n    snowpark_df = session.create_dataframe(final_dataframe).select(col(\"FILE_NAME\"), col(\"TEXT\"))\n    snowpark_df.write.mode(\"overwrite\").save_as_table(\"docs_text_table\")\n\n    \ndef init_messages():\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n    if \"last_request_time\" not in st.session_state:\n        st.session_state.last_request_time = 0\n    if \"carbon_price\" not in st.session_state:\n        st.session_state.carbon_price = fetch_live_carbon_price()\n        write_live_price_to_stage(st.session_state.carbon_price)\n        reindex_documents()\n\ndef init_config_options():\n    st.session_state.num_chat_messages = history_length\n    col1, col2 = st.columns([3, 1])\n    with col1:\n        if st.button(\"Clear conversation\"):\n            st.session_state.messages = []\n            st.rerun()\n    with col2:\n        st.caption(f\"üí∂ Live EU ETS price: ‚Ç¨{st.session_state.carbon_price:.2f}/tCO‚ÇÇe\")\n\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.markdown(message[\"content\"])\n\ndef get_chat_history():\n    start_index = max(0, len(st.session_state.messages) - st.session_state.num_chat_messages)\n    return st.session_state.messages[start_index : len(st.session_state.messages) - 1]\n\ndef extract_cbam_request(question):\n    match = re.search(r\"(\\d+)\\s+tons?\\s+of\\s+(\\w+)\", question, re.IGNORECASE)\n    emissions_match = re.search(r\"(\\d+(\\.\\d+)?)\\s*tCO2e\", question, re.IGNORECASE)\n    origin_price_match = re.search(r\"(?:origin|paid).*?‚Ç¨?(\\d+(\\.\\d+)?)\", question, re.IGNORECASE)\n\n    if match:\n        quantity = int(match.group(1))\n        product = match.group(2).lower()\n    else:\n        quantity = None\n        product = None\n\n    emissions = float(emissions_match.group(1)) if emissions_match else None\n    origin_price = float(origin_price_match.group(1)) if origin_price_match else 0.0\n    return product, quantity, emissions, origin_price\n\ndef calculate_cbam_cost(embedded_emissions, origin_carbon_price=0, eu_carbon_price=None):\n    if eu_carbon_price is None:\n        eu_carbon_price = st.session_state.carbon_price\n    cbam_cost = embedded_emissions * (eu_carbon_price - origin_carbon_price)\n    return max(0, cbam_cost)\n\ndef cortex_search(my_question):\n    search_service = (root\n        .databases[database_name]\n        .schemas[schema_name]\n        .cortex_search_services[service_name]\n    )\n    resp = search_service.search(\n        query=my_question,\n        columns=[\"text\", \"file_name\"],\n        limit=num_results\n    )\n    results = json.loads(resp.to_json())[\"results\"]\n    prompt_context = \"\\n\\n\".join([r[\"text\"] for r in results]).replace(\"'\", \"\")\n    file_name = results[0]['file_name'] if results else \"No source\"\n    return prompt_context[:8000], file_name\n\ndef format_chat_history(chat_history):\n    return \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in chat_history[-3:]])\n\ndef create_prompt(user_question):\n    chat_history = get_chat_history()\n    history_str = format_chat_history(chat_history)\n    prompt_context, file_name = cortex_search(user_question)\n\n    prompt = f\"\"\"You are a CBAM (Carbon Border Adjustment Mechanism) specialist. Provide direct, concise answers using the provided documentation.\n\n<context>\n{prompt_context}\n</context>\n\n<chat_history>\n{history_str}\n</chat_history>\n\n<question>\n{user_question}\n</question>\n\n<instructions>\n1. Answer directly from the provided documentation, including the indexed carbon price file.\n2. Cite specific values, formulas, and guidance from the documents.\n3. For CBAM calculations:\n   - Use default emission values from context if actual emissions not provided\n   - Formula: CBAM Cost = (Embedded Emissions tonnes CO2e) √ó (EU ETS price - Carbon Price Paid in Origin)\n4. Keep responses under 150 words unless calculations require detail\n5. Structure: Direct answer ‚Üí Key requirements ‚Üí Limitations (if any)\n6. If info missing: State what's needed clearly\n</instructions>\n\nResponse:\"\"\"\n    return prompt, file_name\n\n\ndef complete_with_retry(model, prompt, retries=2):\n    now = time.time()\n    if now - st.session_state.last_request_time < MIN_REQUEST_INTERVAL:\n        time.sleep(MIN_REQUEST_INTERVAL - (now - st.session_state.last_request_time))\n\n    for attempt in range(retries):\n        try:\n            response = complete(model, prompt)\n            st.session_state.last_request_time = time.time()\n            return response\n        except Exception as e:\n            if attempt < retries - 1:\n                time.sleep(RETRY_DELAY * (attempt + 1))\n            else:\n                st.error(f\"‚ùå Request failed: {str(e)}\")\n                return None\n\n\ndef main():\n    st.title(\"üåç CBAM Calculator & Documentation Assistant\")\n    init_messages()\n    init_config_options()\n    icons = {\"assistant\": \"‚ùÑÔ∏è\", \"user\": \"üë§\"}\n\n    if question := st.chat_input(\"Ask about CBAM calculations, emissions, or requirements...\"):\n        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n\n        with st.chat_message(\"user\", avatar=icons[\"user\"]):  # ‚úÖ colon added\n            st.markdown(question)\n\n        product, quantity, emissions, origin_price = extract_cbam_request(question)\n        if product and quantity:\n            if emissions is None:\n                emissions = DEFAULT_EMISSIONS.get(product)\n            if emissions is not None:\n                total_emissions = emissions * quantity\n                cbam_cost = calculate_cbam_cost(total_emissions, origin_price)\n                response_text = f\"üí∂ Estimated CBAM cost: ‚Ç¨{cbam_cost:.2f} for {quantity} tons of {product} (Emissions: {emissions} tCO‚ÇÇe/ton, Origin price: ‚Ç¨{origin_price}/t)\"\n                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_text})\n                with st.chat_message(\"assistant\", avatar=icons[\"assistant\"]):\n                    st.markdown(response_text)\n                return\n\n        with st.chat_message(\"assistant\", avatar=icons[\"assistant\"]):  # ‚úÖ colon added\n            with st.spinner(\"Analyzing documentation...\"):\n                prompt, file_name = create_prompt(question)\n                response = complete_with_retry(model_name, prompt)\n                if response:\n                    st.markdown(response)\n                    st.caption(f\"üìÑ Source: {file_name}\")\n                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\nif __name__ == \"__main__\":\n    main()\n"
  }
 ]
}